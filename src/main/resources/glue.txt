import sys
import json
import boto3
import logging
import datetime
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.job import Job

# ----------------------------------------
# üöÄ CONFIGURA√á√ÉO DE LOGGER
# ----------------------------------------
def setup_logger():
    logger = logging.getLogger("glue_job")
    logger.setLevel(logging.INFO)
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    return logger


# ----------------------------------------
# üì° LEITURA DE DADOS VIA JDBC
# ----------------------------------------
def load_data_from_mysql(spark, jdbc_url, dbtable, user, password, driver, logger):
    logger.info(f"Lendo dados da tabela `{dbtable}`")
    return spark.read.format("jdbc") \
        .option("url", jdbc_url) \
        .option("dbtable", dbtable) \
        .option("user", user) \
        .option("password", password) \
        .option("driver", driver) \
        .load()


# ----------------------------------------
# üì¨ ENVIO PARA SQS
# ----------------------------------------
def send_rows_to_sqs(rows, sqs_queue_url, logger):
    sqs = boto3.client("sqs")

    for row in rows:
        row_dict = row.asDict()
        for k, v in row_dict.items():
            if isinstance(v, (datetime.date, datetime.datetime)):
                row_dict[k] = v.strftime("%Y-%m-%d")
        try:
            message_body = json.dumps(row_dict)
            response = sqs.send_message(QueueUrl=sqs_queue_url, MessageBody=message_body)
            logger.info(f"Mensagem enviada: ID={response['MessageId']}")
        except Exception as e:
            logger.error(f"Erro ao enviar mensagem: {str(e)}")
            logger.error(f"Conte√∫do: {json.dumps(row_dict)}")


# ----------------------------------------
# üéØ FUN√á√ÉO PRINCIPAL
# ----------------------------------------
def main():
    args = getResolvedOptions(
        sys.argv,
        ['JOB_NAME', 'MYSQL_HOST', 'MYSQL_PORT', 'MYSQL_DB', 'MYSQL_USER', 'MYSQL_PASS', 'MYSQL_TABLE', 'SQS_QUEUE_URL']
    )

    logger = setup_logger()

    # Inicializa contexto Spark e Glue
    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)

    jdbc_url = f"jdbc:mysql://{args['MYSQL_HOST']}:{args['MYSQL_PORT']}/{args['MYSQL_DB']}"
    driver = "com.mysql.jdbc.Driver"

    # Carregar dados
    df = load_data_from_mysql(
        spark,
        jdbc_url=jdbc_url,
        dbtable=args['MYSQL_TABLE'],
        user=args['MYSQL_USER'],
        password=args['MYSQL_PASS'],
        driver=driver,
        logger=logger
    )

    df.printSchema()
    df.show(5)

    rows = df.collect()

    # Enviar dados para o SQS
    send_rows_to_sqs(rows, args['SQS_QUEUE_URL'], logger)

    job.commit()


# ----------------------------------------
# üîÅ EXECU√á√ÉO
# ----------------------------------------
if __name__ == "__main__":
    main()
